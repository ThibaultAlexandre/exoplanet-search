{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/exoTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with classical classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select only the first 5000 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.iloc[:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_data.drop('LABEL',axis = 1))\n",
    "y = np.array(train_data[['LABEL']]).reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class for testing different ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self,X,y):\n",
    "        self.scaler.fit(X)\n",
    "        X = self.scaler.transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        return self.model.score(X, y)\n",
    "\n",
    "    def predict_rescaled(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def good_detection_score(self, X, y, target = 2):\n",
    "        res = self.predict_rescaled(X)\n",
    "        correct_guesses = 0.0\n",
    "        total = 0.0\n",
    "        for i in range(len(res)):\n",
    "            if y[i] == target:\n",
    "                total += 1\n",
    "                if res[i] == target:\n",
    "                    correct_guesses += 1\n",
    "        print('correct guesses : ' + str(correct_guesses))\n",
    "        print('total : ' + str(total))   \n",
    "        print('score : '+str(correct_guesses/total))\n",
    "\n",
    "    def fake_detection_score(self, X, y, target = 2):\n",
    "        res = self.predict_rescaled(X)\n",
    "        fake_guesses = 0.0\n",
    "        total = 0.0\n",
    "        for i in range(len(res)):\n",
    "            if y[i] != target:\n",
    "                total += 1\n",
    "                if res[i] == target:\n",
    "                    fake_guesses += 1\n",
    "        print('fake guesses : ' + str(fake_guesses))\n",
    "        print('total : ' + str(total))\n",
    "        print('score : '+str(fake_guesses/total))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with the train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Classifier(RandomForestClassifier())\n",
    "model2 = Classifier(SVC())\n",
    "score1 = model1.train(X,y)\n",
    "score2 = model2.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good detection score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.good_detection_score(X,y)\n",
    "model2.good_detection_score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake detection score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fake_detection_score(X,y)\n",
    "model2.fake_detection_score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with classical classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/exoTest.csv')\n",
    "X_test = np.array(test_data.drop('LABEL',axis = 1))\n",
    "y_test = np.array(test_data[['LABEL']]).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.good_detection_score(X_test,y_test)\n",
    "model1.fake_detection_score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.good_detection_score(X_test,y_test)\n",
    "model2.fake_detection_score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_nn(nn.Module):\n",
    "    def __init__(self,input_size, kernel_number, sequence_length, kernel_size, stride = 1, average_size = 20, output_size = 1):\n",
    "        super(CNN_nn, self).__init__()\n",
    "        #Attributes\n",
    "        self.input_size = input_size #input size\n",
    "        self.output_size = output_size #output size\n",
    "        self.kernel_number = kernel_number #number of kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.average_size = average_size\n",
    "        self.sequence_length = sequence_length-self.average_size+1 #length of the 1d input sequence\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler_target = MinMaxScaler()\n",
    "\n",
    "        self.cnn1 = nn.Conv1d(self.input_size,self.kernel_number,self.kernel_size,self.stride) #Conv 1d\n",
    "        self.max_pool1 = nn.MaxPool1d(self.kernel_size) #Max pooling\n",
    "        self.cnn2 = nn.Conv1d(self.kernel_number,self.kernel_number,self.kernel_size,self.stride) #Conv 1d\n",
    "        self.max_pool2 = nn.MaxPool1d(self.kernel_size) #Max pooling\n",
    "\n",
    "        example = self.cnn1(torch.randn(1,self.input_size,self.sequence_length)) # one batch to get the output length dimension\n",
    "        example = self.max_pool1(example)\n",
    "        example = self.cnn2(example)\n",
    "        example = self.max_pool2(example)\n",
    "        self.Lout = example.size(2) #Length of the output sequence\n",
    "\n",
    "        self.fc = nn.Linear(self.kernel_number*self.Lout, self.output_size) #fully connected linear\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def smoothening(self,X):\n",
    "        rows = X.shape[0]\n",
    "        columns = X.shape[1]\n",
    "        res = np.zeros((rows,columns-self.average_size+1))\n",
    "        for j in range(rows):\n",
    "            for i in range(columns-self.average_size+1):\n",
    "                res[j,i] = sum(X[j,i:i+self.average_size])/self.average_size\n",
    "        return res\n",
    "\n",
    "    def process_features(self, X):\n",
    "        #Standardisation\n",
    "        X_torch = self.scaler.transform(X)\n",
    "\n",
    "        #Smoothening with moving average\n",
    "        X_torch = self.smoothening(X_torch)\n",
    "        \n",
    "        #Reshaping\n",
    "        X_torch = Variable(torch.Tensor(X_torch))\n",
    "        X_torch = X_torch.reshape(-1,1,self.sequence_length)\n",
    "        return X_torch\n",
    "\n",
    "    def process_target(self, y):\n",
    "        y_torch = self.scaler_target.transform(y)\n",
    "        y_torch = Variable(torch.Tensor(y_torch))\n",
    "        return y_torch\n",
    "\n",
    "    def forward(self,X):\n",
    "        #Convolution layer\n",
    "        out = self.cnn1(X)\n",
    "        #Activation with relu\n",
    "        out = self.relu(out)\n",
    "        #Max pooling\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        #Convolution layer\n",
    "        out = self.cnn2(out)\n",
    "        #Activation with relu\n",
    "        out = self.relu(out)\n",
    "        #Max pooling\n",
    "        out = self.max_pool2(out)\n",
    "        \n",
    "        # Flatten the output for fully connected layer\n",
    "        out = out.flatten(1,2)\n",
    "\n",
    "        # Propagate input through fully connected linear neuron\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # Activation with sigmoid\n",
    "        out = self.sig(out)\n",
    "        return out\n",
    "    \n",
    "    def forward_with_processing(self,X):\n",
    "        X_torch = self.process_features(X)\n",
    "        return self.forward(X_torch)\n",
    "    \n",
    "    def train(self, num_epochs, learning_rate, criterion, X_train, y_train, X_test, y_test):\n",
    "        self.scaler.fit(X_train)\n",
    "        self.scaler_target.fit(y_train)\n",
    "        \n",
    "        X_train = self.process_features(X_train)\n",
    "        y_train = self.process_target(y_train)\n",
    "\n",
    "        X_test = self.process_features(X_test)\n",
    "        y_test = self.process_target(y_test)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate) \n",
    "        t = trange(num_epochs+1)\n",
    "        for epoch in t:\n",
    "            #Pass through the neural network\n",
    "            train_outputs = self.forward(X_train) \n",
    "            test_outputs = self.forward(X_test) \n",
    "\n",
    "            #Reset gradients to zero  \n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            train_loss = criterion(train_outputs, y_train)\n",
    "            test_loss = criterion(test_outputs,y_test)\n",
    "\n",
    "            #Backprogagation step\n",
    "            train_loss.backward()\n",
    "\n",
    "            #Update weights and bias of the network\n",
    "            optimizer.step()\n",
    "\n",
    "            #Print train and test loss\n",
    "            t.set_description(\"Epoch: %d, Train loss: %1.5f, Test loss: %1.5f\" % (epoch, train_loss.item(),test_loss.item()))\n",
    "\n",
    "    def good_detection_score(self, X, y, target = 1.0, epsilon = 0.01):\n",
    "            y_predicted = self.forward_with_processing(X)\n",
    "            y_ref = self.process_target(y)\n",
    "            correct_guesses = 0.0\n",
    "            total = 0.0\n",
    "            for i in range(len(y_predicted)):\n",
    "                if abs(y_ref[i]-target) < epsilon:\n",
    "                    total += 1\n",
    "                    if abs(y_predicted[i]-target) < epsilon:\n",
    "                        correct_guesses += 1\n",
    "            print('correct guesses : ' + str(correct_guesses))\n",
    "            print('total : ' + str(total))   \n",
    "            print('score : '+str(correct_guesses/total))\n",
    "\n",
    "    def fake_detection_score(self, X, y, target = 1.0, epsilon = 0.01):\n",
    "        y_predicted = self.forward_with_processing(X)\n",
    "        y_ref = self.process_target(y)\n",
    "        fake_guesses = 0.0\n",
    "        total = 0.0\n",
    "        for i in range(len(y_predicted)):\n",
    "            if abs(y_ref[i]-target) > epsilon:\n",
    "                total += 1\n",
    "                if abs(y_predicted[i]-target) < epsilon:\n",
    "                    fake_guesses += 1\n",
    "        print('fake guesses : ' + str(fake_guesses))\n",
    "        print('total : ' + str(total))\n",
    "        print('score : '+str(fake_guesses/total))\n",
    "\n",
    "    def fourier_transform(self,X):\n",
    "        FT = np.fft.fft(X,axis = 1)\n",
    "        FT_norm = abs(FT)\n",
    "        return FT_norm\n",
    "\n",
    "    def plot_features(self, X, cut_low= 100, cut_high = 100):\n",
    "        X_torch = X.reshape(1,-1)\n",
    "        X_torch = self.smoothening(X_torch)\n",
    "        FFT_norm = self.fourier_transform(X_torch)\n",
    "\n",
    "        plt.clf()\n",
    "        plot1 = plt.figure(1)\n",
    "        plt.plot(X)\n",
    "\n",
    "        plot2 = plt.figure(2)\n",
    "        plt.plot(X_torch[0])\n",
    "\n",
    "        plot3 = plt.figure(3)\n",
    "        plt.plot(FFT_norm[0,cut_low:len(FFT_norm)-cut_high-1])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/exoTrain.csv')\n",
    "sequence_length = 600\n",
    "number_of_stars = 100\n",
    "train_data = train_data.iloc[:number_of_stars,:sequence_length+1]\n",
    "X_train = np.array(train_data.drop('LABEL',axis = 1))\n",
    "y_train = np.array(train_data[['LABEL']]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/exoTest.csv')\n",
    "number_of_stars = 2000\n",
    "test_data = test_data.iloc[:number_of_stars,:sequence_length+1]\n",
    "X_test = np.array(test_data.drop('LABEL',axis = 1))\n",
    "y_test = np.array(test_data[['LABEL']]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = CNN_nn(input_size=1, kernel_number = 16, sequence_length=sequence_length, kernel_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.002\n",
    "criterion = nn.BCELoss()\n",
    "model3.train(num_epochs, learning_rate, criterion, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training')\n",
    "model3.good_detection_score(X_train,y_train)\n",
    "model3.fake_detection_score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing')\n",
    "model3.good_detection_score(X_test,y_test)\n",
    "model3.fake_detection_score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.plot_features(X_train[70], cut_low=100, cut_high=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bc153b11e6b6b4ba09590905be52005545a2fd1d59041e11565ebc3908239f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
